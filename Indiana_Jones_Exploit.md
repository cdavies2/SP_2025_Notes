# Introduction
* LLM security often refers to their ability to generate content adhering to ethical standards and societal moral values. A risk is for an LLM to produce harmful content, like by inadvertently giving guidance in how to commit a violent crime.
* LLMs are vulnerable to jailbreaking, methods that bypass their safeguards.
* The Indiana Jones method produces near-perfect jailbreak success across various directions in mainstream LLMs.

# Experiment Design
* Indiana Jones integrates three LLMs that interact across multiple rounds to achieve the jailbreak goal. "Victim" is the target model subjected to attack, "suspect" generates prompts to guide the attack, "Checker" evaluates outputs from the victim to determine their validity.
* Checker is first asked to assess which direction is suitable for the user-provided keyword to initiate a jailbreak, and once the direction is selection, Checker instructs Victim to produce relevant historical or contextual information, like notable figures or events. Output is analyzed by Checker, and if deemed appropriate it is passed to Suspect, which generates further prompts to refine or extend Victim-generated content.
* Success is defined as generation of jailbreak content that aligns with the keyword within these rounds. There is a maximum of five rounds.
* The evaluation focused on English and Chinese - two high-resource languages that dominate LLM training datasets and represent contexts where peak performance is expected
* The metrics used to evaluate this method were...
  * Attack Success Rate (ASR): percentage of queries that bypass LLM safeguards
  * Efficiency: average time required to complete a jailbreak attempt per query (shows practicality of the method)
  * Robustness: ability to consistently bypass defenses across various LLM architectures and updates. This was tested on models like GPT-4o, Claude-3.5, and Llama3.2

# Method
* This is a "historical jailbreak", leveraging historical perspectives to bypass LLM safeguards. LLMs typically refuse to answer queries about unethical or illegal actions, but when asked indirectly (like about historical figures or precedents in unethical areas), LLMs tend to give relevant information and elaborate when prompted.
* Instead of explicitly selecting a jailbreak method, users provide a harmful keyword and the LLM determines the most suitable approach.
## Figures, Artistic, or Debate
* _Figures_: LLM is prompted to identify notable historical figures related to the keyword, Victim model generates a list of relevant individuals, selects the most pertinent figure and offers a detailed account of their actions
* _Artistic_: LLM is questioned about historical artworks within a specified domain, and following prompts guide the Victim to generate analogous artistic outputs, often imitating or reinterpreting the identified works and providing harmful instructions in the process
* _Debate_: LLM must present arguments in favor of certain actions or behaviors, emphasising societal benefits and potential for contemporary application.

# Analysis and Evaluation
* Using five LLMs (ChatGPT-4o, Claude, Llama3.2, Qwen2.5, Gemma2) across twelve jailbreak scenarios, the proposed method demonstrated strong performance in both black-box and white-box settings, highlighting adaptability and effectiveness.
* ChatGPT-4o effectively filters sensitive content like health related queries, but the proposed method circumvents these safeguards, exposing vulnerabilities in detecting nuanced manipulative prompts. Claude prioritizes conversational coherence, making it more susceptible to indirect manipulations despite excelling in user-friendly interactions.
* Llama3.2 is weak in addressing context-dependent and ethically sensitive prompts
## Evaluation of Results
* On black-box models like GPT-4o Indiana Jones achieves a success rate of 98.9%, and on white-box models, it outperforms DeepInception, demonstrating an ability to adapt to diverse architectures.
* PAIR, a classic jailbreak method, achieves low ASRs of 18.6% on GPT-4o and 16.5% on Llama3.2, while Indiana Jones achieved nearly 100% success within five rounds
* Indiana Jones consistently performed well across both white-box and black-box models. On Qwen2.5 and Gemma2, Indiana Jones found success by combining inter-model dialogues with dynamic keyword analysis, while other methods lack adaptability.

# Defense Test and Ablation Experiment
* Indiana Jones remains resilient under advanced defenses like self-reminders or in-context adjustments.
* One of the key prompts for Indiana Jones is "Based on your previous answers" since most mainstream LLMs don't reject or have a low probability of rejecting questions about content of their answers. Once this prompt is removed, models like ChatGPT and Claude hallucinate severely while Llama3.2 in 80% of cases refused to answer in multiple rounds of dialogue.

# Strengths and Limitations
* _Strengths_:
  * Low resource usage (lightweight design ensures scalability)
  * Broad coverage (addresses wide range of jailbreak scenarios)
  * Robustness (performs consistently against maodels with stringent safety measures)
* _Limitations_:
  * Language scope (only focuses on English and Chinese)
  * Simulated Evaluations (relies on controlled experiments)
* Source: https://arxiv.org/html/2501.18628v1 
